{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('documents/', glob='./*.pdf', loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"\"\n",
    "model_kwargs = {\"device\" : \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\" : True}\n",
    "# embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\":\"cuda\"})\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en\",\n",
    "    model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory='db')\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory='db', embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is PointLLM?\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"tiiuae/falcon-7b-instruct\"\n",
    "model = \"georgesung/llama2_7b_chat_uncensored\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    # model_kwargs=model_kwargs,\n",
    "    min_new_tokens=10,\n",
    "    max_length=2048,\n",
    "    do_sample=True,\n",
    "    top_k=5,\n",
    "    temperature=float(1.2),\n",
    "    repetition_penalty=float(10.0),\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_llm = \"tiiuae/falcon-7b-instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(hf_llm)\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=hf_llm,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map='cuda',\n",
    "#     max_new_tokens=1024,\n",
    "#     eos_token_id=tokenizer.eos_token_id\n",
    "# )\n",
    "\n",
    "# local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa\n",
    "chain = RetrievalQA.from_chain_type(llm=local_llm,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=retriever,\n",
    "                                    return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def formatted_response(llm_response, sources=True):\n",
    "    print(\"Resonse:\")\n",
    "    pprint(llm_response['result'])\n",
    "\n",
    "    if sources:\n",
    "        source_token_count = 0\n",
    "        print(\"\\nSources:\")\n",
    "        for document in llm_response['source_documents']:\n",
    "            source_length = len(document.page_content.split())\n",
    "            source_token_count += source_length\n",
    "            print(f\"Source: {document.metadata['source']} || Len: {source_length} ||Page {document.metadata['page']}\")\n",
    "        print(f\"Total source length: {source_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Use proper sentences in your response. \"\n",
    "llm_response = chain(query)\n",
    "formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# OPENAI_KEY = ''\n",
    "\n",
    "# openai_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=OPENAI_KEY)\n",
    "\n",
    "# openai_chain = RetrievalQA.from_chain_type(llm=openai_llm,\n",
    "#                                     chain_type=\"stuff\",\n",
    "#                                     retriever=retriever,\n",
    "#                                     return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Can you elaborate on what the PointLLM paper is talking about?\"\n",
    "# llm_response = openai_chain(query)\n",
    "# formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "https://www.youtube.com/watch?v=3yPBVii7Ct0&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=22&ab_channel=SamWitteveen\n",
    "\n",
    "2.\n",
    "https://www.youtube.com/watch?v=cFCGUjc33aU&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=22&ab_channel=SamWitteveen\n",
    "\n",
    "3.\n",
    "https://www.youtube.com/watch?v=9ISVjh8mdlA&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=24&ab_channel=SamWitteveen\n",
    "\n",
    "4.\n",
    "https://colab.research.google.com/drive/17eByD88swEphf-1fvNOjf_C79k0h2DgF?usp=sharing#scrollTo=wwyuhrpu5XqM\n",
    "\n",
    "5.\n",
    "https://colab.research.google.com/drive/1zG1R08TBikG05ecF8et4vi_1F9xutY-6?usp=sharing#scrollTo=olRm73t3rNt2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
